{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b6ea39",
   "metadata": {},
   "source": [
    "# Advanced Credit Card Fraud Detection using RNN\n",
    "\n",
    "This notebook implements an advanced credit card fraud detection system using Recurrent Neural Networks (RNN) with TensorFlow. We'll use LSTM layers to capture sequential patterns in credit card transactions and build a robust fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e256a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's import all the necessary libraries for our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d740430",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Let's load the credit card fraud dataset and perform initial preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Class'].value_counts(normalize=True))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Create sequences for RNN (using time steps of 3)\n",
    "def create_sequences(X, y, time_steps=3):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Create sequences\n",
    "X_seq, y_seq = create_sequences(X_scaled, y)\n",
    "\n",
    "print(\"\\nSequence shape:\", X_seq.shape)\n",
    "print(\"Target shape:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f61768",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Let's analyze the data distribution and handle class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='Class')\n",
    "plt.title('Class Distribution (Before SMOTE)')\n",
    "plt.show()\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_seq_reshaped = X_seq.reshape(X_seq.shape[0], -1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_seq_reshaped, y_seq)\n",
    "X_resampled = X_resampled.reshape(-1, X_seq.shape[1], X_seq.shape[2])\n",
    "\n",
    "print(\"Original shape:\", X_seq.shape)\n",
    "print(\"Resampled shape:\", X_resampled.shape)\n",
    "\n",
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(data=df, x=f'V{i+1}', hue='Class', bins=50)\n",
    "    plt.title(f'Distribution of V{i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fff33",
   "metadata": {},
   "source": [
    "## 4. Building RNN Model\n",
    "\n",
    "Let's create our advanced RNN model using LSTM layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the advanced RNN model with 6 hidden layers\n",
    "model = Sequential([\n",
    "    # 1st LSTM layer with tanh activation (default for LSTM)\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # 2nd LSTM layer with tanh activation\n",
    "    LSTM(96, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # 3rd LSTM layer with tanh activation\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # 4th LSTM layer with tanh activation\n",
    "    LSTM(48, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # 5th LSTM layer with tanh activation\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # 6th LSTM layer\n",
    "    LSTM(24, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Dense layers with different activation functions\n",
    "    Dense(64, activation='relu'),  # ReLU activation\n",
    "    Dense(48, activation='elu'),   # ELU activation\n",
    "    Dense(32, activation='selu'),  # SELU activation\n",
    "    Dense(24, activation='tanh'),  # Tanh activation\n",
    "    Dense(16, activation='swish'), # Swish activation\n",
    "    Dense(8, activation='sigmoid'), # Sigmoid activation\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Binary classification loss function\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(), \n",
    "                      tf.keras.metrics.Precision(), \n",
    "                      tf.keras.metrics.Recall()])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37eb77a",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Let's train our model with early stopping to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768098a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Increased epochs for complex model\n",
    "    batch_size=64,  # Adjusted batch size\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot additional metrics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Precision\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['precision'], label='Training Precision')\n",
    "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
    "plt.title('Model Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Recall\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['recall'], label='Training Recall')\n",
    "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
    "plt.title('Model Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a9bf",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance using various metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc9c00",
   "metadata": {},
   "source": [
    "## 7. Save and Load Model\n",
    "\n",
    "Let's save our trained model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401635d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler\n",
    "model.save('fraud_detection_model.h5')\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model and scaler saved successfully!\")\n",
    "\n",
    "# Function to load the model and scaler\n",
    "def load_model_and_scaler():\n",
    "    loaded_model = tf.keras.models.load_model('fraud_detection_model.h5')\n",
    "    with open('scaler.pkl', 'rb') as f:\n",
    "        loaded_scaler = pickle.load(f)\n",
    "    return loaded_model, loaded_scaler\n",
    "\n",
    "# Test loading\n",
    "loaded_model, loaded_scaler = load_model_and_scaler()\n",
    "print(\"Model and scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff49046",
   "metadata": {},
   "source": [
    "## 8. Real-time Prediction\n",
    "\n",
    "Let's create a function for real-time fraud detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b97ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fraud(transaction_data, model, scaler, time_steps=3):\n",
    "    \"\"\"\n",
    "    Predict if a transaction is fraudulent\n",
    "    \n",
    "    Parameters:\n",
    "    transaction_data: numpy array of shape (30,) containing the transaction features\n",
    "    model: loaded keras model\n",
    "    scaler: loaded StandardScaler\n",
    "    time_steps: number of time steps used in training\n",
    "    \n",
    "    Returns:\n",
    "    probability of fraud, prediction (0: legitimate, 1: fraud)\n",
    "    \"\"\"\n",
    "    # Scale the transaction\n",
    "    scaled_transaction = scaler.transform(transaction_data.reshape(1, -1))\n",
    "    \n",
    "    # Create sequence (assuming we have previous transactions)\n",
    "    # For demo, we'll just repeat the same transaction\n",
    "    sequence = np.repeat(scaled_transaction, time_steps, axis=0)\n",
    "    sequence = sequence.reshape(1, time_steps, -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    fraud_probability = model.predict(sequence)[0][0]\n",
    "    fraud_prediction = 1 if fraud_probability > 0.5 else 0\n",
    "    \n",
    "    return fraud_probability, fraud_prediction\n",
    "\n",
    "# Test the function with a sample transaction\n",
    "sample_transaction = X.iloc[0].values\n",
    "prob, pred = predict_fraud(sample_transaction, loaded_model, loaded_scaler)\n",
    "\n",
    "print(f\"Fraud Probability: {prob:.2%}\")\n",
    "print(f\"Prediction: {'Fraudulent' if pred == 1 else 'Legitimate'} Transaction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
